---
title: "TreeBasedMethods"
author: "Yunsheng Lu"
date: "2023-04-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(fastDummies)
library(tidyverse)
library(caret)
library(MASS)
# read data ------------------------
math = read.csv("student-mat.csv", sep = ';', stringsAsFactors = T)
str(math)
por = read.csv("student-por.csv", sep = ";", stringsAsFactors = T)
str(por)

#set dummies
math = fastDummies::dummy_columns(math, remove_first_dummy = T, remove_selected_columns = T)
por = fastDummies::dummy_columns(por, remove_first_dummy = T, remove_selected_columns = T)


predictors = setdiff(colnames(math), c("G1", "G2", "G3"))

math$grade.cat = cut(math$G1, breaks=c(-1, 9, 11, 13, 15, 20),
                     labels=c('F', 'D', 'C', 'B', 'A'))
por$grade.cat = cut(por$G1, breaks=c(-1, 9, 11, 13, 15, 20),
                    labels=c('F', 'D', 'C', 'B', 'A'))
por$grade.con = por$G1
math$grade.con = math$G1


# split data -------------------------
set.seed(114514)
train_idx.math = sample(1:nrow(math), floor(nrow(math))*0.8, replace = F)
train_idx.por = sample(1:nrow(por), floor(nrow(por))*0.8, replace = F)

math.train = math[train_idx.math,]
math.test = math[-train_idx.math,]

por.train = por[train_idx.por,]
por.test = por[-train_idx.por,]

```

Histogram
```{r}
par(mfrow=c(2,2))
hist(as.numeric(math.train$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Math Train")
hist(as.numeric(math.test$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Math Test")
hist(as.numeric(por.train$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Por Train")
hist(as.numeric(por.test$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Por Test")
```

```{r}
set.seed(114514)
train_idx.math = sample(1:nrow(math), floor(nrow(math))*0.8, replace = F)
train_idx.por = sample(1:nrow(por), floor(nrow(por))*0.8, replace = F)

math.train = math[train_idx.math,]
math.test = math[-train_idx.math,]

por.train = por[train_idx.por,]
por.test = por[-train_idx.por,]
preproc.param <- math.train %>%
  preProcess(method = c("center", "scale"))
math.train <- preproc.param %>% predict(math.train)
math.test <- preproc.param %>% predict(math.test)
preproc.param <- por.train %>%
  preProcess(method = c("center", "scale"))
por.train <- preproc.param %>% predict(por.train)
por.test <- preproc.param %>% predict(por.test)

library(e1071)
classification.formula = as.formula(paste("grade.cat~", paste(predictors, collapse = "+"), sep = ''))
regression.formula = as.formula(paste("grade.con~", paste(predictors, collapse = "+"), sep = ''))
```

## Tree-based Regression------------

```{r}
library(rpart.plot)
library(rattle)
library(rpart)
library(caret)
library(randomForest)
library(gbm)
## Decision Tree for math---------
### Unpruned Tree----------
tree.math<- rpart(regression.formula, data=math.train,control=rpart.control(cp=0))
#plot the tree
fancyRpartPlot(tree.math, caption = NULL)
#predicted result for tree
tree.pred.math<-predict(tree.math,newdata=,math.test)
tree.pred.math
#MSE for unpruned
mean((tree.pred.math-math.test$grade.con)^2)
### Pruned Tree-------
set.seed(114514)
tree.tune.math <- train(regression.formula, 
                        data = math.train, 
                        method = "rpart", 
                        preProcess = c("center", "scale"),
                        tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                        trControl = trainControl(
                          method = "repeatedcv",
                          repeats = 5, number = 10
                        )
)
plot(tree.tune.math)
#best cost parameter
best<-tree.tune.math$bestTune
best
tree.math.pruned<- rpart(regression.formula, data=math.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.math.pruned, caption = NULL)
#predicted result for pruned tree
tree.pruned.math.pred<-predict(tree.math.pruned,newdata=math.test)
tree.pruned.math.pred
#MSE for pruned
mean((tree.pruned.math.pred-math.test$grade.con)^2)
tree.por<- rpart(regression.formula, data=por.train,control=rpart.control(cp=0))


## Decision Tree for Por---------
#plot the tree
fancyRpartPlot(tree.por, caption = NULL)
#predicted result for tree
tree.pred.por<-predict(tree.por,newdata=por.test)
tree.pred.por
#MSE for unpruned
mean((tree.pred.por-por.test$grade.con)^2)
### Pruned Tree-------
set.seed(114514)
tree.tune.por <- train(regression.formula, 
                       data = por.train, 
                       method = "rpart", 
                       preProcess = c("center", "scale"),
                       tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                       trControl = trainControl(
                         method = "repeatedcv",
                         repeats = 5, number = 10
                       )
)
plot(tree.tune.por)
#best cost parameter
best<-tree.tune.por$bestTune
best
tree.por.pruned<- rpart(regression.formula, data=por.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.por.pruned, caption = NULL)
#predicted result for pruned tree
tree.pruned.por.pred<-predict(tree.por.pruned,newdata=,por.test)
tree.pruned.por.pred
#MSE
mean((tree.pruned.por.pred-por.test$grade.con)^2)
```

Random Forrest regression
```{r}
library(randomForest)
### math--------
#choice of m
sqrt(42)
set.seed(114514)
train.rand.for.math<-randomForest(regression.formula,data=math.train,mtry=6)
train.rand.for.math
test.pred.rand.for.math<-predict(train.rand.for.math,newdata=math.test)
#test result
test.pred.rand.for.math
#misclassifictaion rate
mean((test.pred.rand.for.math-math.test$grade.con)^2)
library(vip)
vip::vip(train.rand.for.math)
### por---------
### por--------
#choice of m
sqrt(42)
set.seed(114514)
train.rand.for.por<-randomForest(regression.formula,data=por.train,mtry=6)
train.rand.for.por
test.pred.rand.for.por<-predict(train.rand.for.por,newdata=por.test)
#test result
test.pred.rand.for.por
#misclassifictaion rate
mean((test.pred.rand.for.por-por.test$grade.con)^2)
library(vip)
vip::vip(train.rand.for.por)
```

TreeBased Classification

```{r}
library(rpart.plot)
library(rattle)
library(rpart)
library(caret)
library(gbm)
## Decision Tree for math---------
### Unpruned Tree----------
tree.math<- rpart(classification.formula, data=math.train,control=rpart.control(cp=0))
#plot the tree
#predicted result for tree
tree.pred.math<-predict(tree.math,newdata=,math.test,type="class")
tree.pred.math
#misclassification for unpruned
1-mean(tree.pred.math==math.test$grade.cat)
### Pruned Tree-------
set.seed(114514)
tree.tune.math <- train(classification.formula, 
                    data = math.train, 
                  method = "rpart", 
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cp = seq(0, 0.05, by=0.001)),
                  trControl = trainControl(
                    method = "repeatedcv",
                    repeats = 5, number = 10
                    )
                 )
plot(tree.tune.math)
#best cost parameter
best<-tree.tune.math$bestTune
best
tree.math.pruned<- rpart(classification.formula, data=math.train,control=rpart.control(cp=best))
## We can't plot this because it's only a root--------
#predicted result for pruned tree
tree.pruned.math.pred<-predict(tree.math.pruned,newdata=math.test,type="class")
tree.pruned.math.pred
#misclassification for pruned
1-mean(tree.pruned.math.pred==math.test$grade.cat)
tree.por<- rpart(classification.formula, data=por.train,control=rpart.control(cp=0))


## Decision Tree for Por---------
#plot the tree
#predicted result for tree
tree.pred.por<-predict(tree.por,newdata=,por.test,type="class")
tree.pred.por
#misclassification rate for unpruned
1-mean(tree.pred.por==por.test$grade.cat)
### Pruned Tree-------
set.seed(114514)
tree.tune.por <- train(classification.formula, 
                        data = por.train, 
                        method = "rpart", 
                        preProcess = c("center", "scale"),
                        tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                        trControl = trainControl(
                          method = "repeatedcv",
                          repeats = 5, number = 10
                        )
)
plot(tree.tune.por)
#best cost parameter
best<-tree.tune.por$bestTune
best
tree.por.pruned<- rpart(classification.formula, data=por.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.por.pruned, caption = NULL)
#predicted result for pruned tree
tree.pruned.por.pred<-predict(tree.por.pruned,newdata=por.test,type="class")
tree.pruned.por.pred
#misclassification rate for pruned
1-mean(tree.pruned.por.pred==por.test$grade.cat)
```

Random Forrest Classification
```{r}
library(randomForest)
### math--------
#choice of m
sqrt(42)
sequence<-seq(20,200,by=10)
set.seed(114514)
train.rand.for.math<-randomForest(classification.formula,data=math.train,mtry=6)
train.rand.for.math
test.pred.rand.for.math<-predict(train.rand.for.math,math.test,type="class")
#test result
test.pred.rand.for.math
#misclassifictaion rate
1-mean(test.pred.rand.for.math==math.test$grade.cat)
library(vip)
vip::vip(train.rand.for.math)
### por---------
### por--------
#choice of m
sqrt(42)
sequence<-seq(20,200,by=10)
set.seed(114514)
train.rand.for.por<-randomForest(classification.formula,data=por.train,mtry=6)
train.rand.for.por
test.pred.rand.for.por<-predict(train.rand.for.por,por.test,type="class")
#test result
test.pred.rand.for.por
#misclassifictaion rate
1-mean(test.pred.rand.for.por==por.test$grade.cat)
library(vip)
vip::vip(train.rand.for.por)
```


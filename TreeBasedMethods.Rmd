---
title: "TreeBasedMethods"
author: "Yunsheng Lu"
date: "2023-04-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(fastDummies)
library(tidyverse)
library(caret)
library(MASS)
# read data ------------------------
math = read.csv("student-mat.csv", sep = ';', stringsAsFactors = T)
str(math)
por = read.csv("student-por.csv", sep = ";", stringsAsFactors = T)
str(por)

#set dummies
math = fastDummies::dummy_columns(math, remove_first_dummy = T, remove_selected_columns = T)
por = fastDummies::dummy_columns(por, remove_first_dummy = T, remove_selected_columns = T)


predictors = setdiff(colnames(math), c("G1", "G2", "G3"))

math$grade.cat = cut(math$G1, breaks=c(-1, 9, 11, 13, 15, 20),
                     labels=c('F', 'D', 'C', 'B', 'A'))
por$grade.cat = cut(por$G1, breaks=c(-1, 9, 11, 13, 15, 20),
                    labels=c('F', 'D', 'C', 'B', 'A'))
por$grade.con = por$G1
math$grade.con = math$G1


# split data -------------------------
set.seed(114514)
train_idx.math = sample(1:nrow(math), floor(nrow(math))*0.8, replace = F)
train_idx.por = sample(1:nrow(por), floor(nrow(por))*0.8, replace = F)

math.train = math[train_idx.math,]
math.test = math[-train_idx.math,]

por.train = por[train_idx.por,]
por.test = por[-train_idx.por,]

```

Histogram
```{r}
par(mfrow=c(2,2))
hist(as.numeric(math.train$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Math Train")
hist(as.numeric(math.test$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Math Test")
hist(as.numeric(por.train$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Por Train")
hist(as.numeric(por.test$grade.cat),breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5),right=TRUE,freq=TRUE,main="Por Test")
```

```{r}
set.seed(114514)
train_idx.math = sample(1:nrow(math), floor(nrow(math))*0.8, replace = F)
train_idx.por = sample(1:nrow(por), floor(nrow(por))*0.8, replace = F)

math.train = math[train_idx.math,]
math.test = math[-train_idx.math,]

por.train = por[train_idx.por,]
por.test = por[-train_idx.por,]
preproc.param <- math.train %>%
  preProcess(method = c("center", "scale"))
math.train <- preproc.param %>% predict(math.train)
math.test <- preproc.param %>% predict(math.test)
preproc.param <- por.train %>%
  preProcess(method = c("center", "scale"))
por.train <- preproc.param %>% predict(por.train)
por.test <- preproc.param %>% predict(por.test)

library(e1071)
classification.formula = as.formula(paste("grade.cat~", paste(predictors, collapse = "+"), sep = ''))
regression.formula = as.formula(paste("grade.con~", paste(predictors, collapse = "+"), sep = ''))
```

## Tree-based Regression------------

```{r}
library(rpart.plot)
library(rattle)
library(rpart)
library(caret)
library(randomForest)
library(gbm)
## Decision Tree for math---------
### Unpruned Tree----------
tree.math<- rpart(regression.formula, data=math.train,control=rpart.control(cp=0))
#predicted result for tree
train.math<-predict(tree.math,newdata=math.train)
tree.pred.math<-predict(tree.math,newdata=math.test)
tree.pred.math
#MSE for unpruned
mean((train.math-math.train$grade.con)^2)
mean((tree.pred.math-math.test$grade.con)^2)
### Pruned Tree-------
set.seed(114514)
tree.tune.math <- train(regression.formula, 
                        data = math.train, 
                        method = "rpart", 
                        preProcess = c("center", "scale"),
                        tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                        trControl = trainControl(
                          method = "repeatedcv",
                          repeats = 5, number = 10
                        )
)
plot(tree.tune.math)
#best cost parameter
best<-tree.tune.math$bestTune
best
tree.math.pruned<- rpart(regression.formula, data=math.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.math.pruned, caption = NULL)
#predicted result for pruned tree
train.math<-predict(tree.math.pruned,newdata=math.train)
tree.pruned.math.pred<-predict(tree.math.pruned,newdata=math.test)
tree.pruned.math.pred
#MSE for pruned
mean((train.math-math.train$grade.con)^2)
mean((tree.pruned.math.pred-math.test$grade.con)^2)




## Decision Tree for Por---------
tree.por<- rpart(regression.formula, data=por.train,control=rpart.control(cp=0))
#predicted result for tree
train.por<-predict(tree.por,newdata=por.train)
tree.pred.por<-predict(tree.por,newdata=por.test)
tree.pred.por

#MSE for unpruned
mean((train.por-por.train$grade.con)^2)
mean((tree.pred.por-por.test$grade.con)^2)
### Pruned Tree-------
set.seed(114514)
tree.tune.por <- train(regression.formula, 
                       data = por.train, 
                       method = "rpart", 
                       preProcess = c("center", "scale"),
                       tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                       trControl = trainControl(
                         method = "repeatedcv",
                         repeats = 5, number = 10
                       )
)
plot(tree.tune.por)
#best cost parameter
best<-tree.tune.por$bestTune
best
tree.por.pruned<- rpart(regression.formula, data=por.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.por.pruned, caption = NULL)
#predicted result for pruned tree
train.por<-predict(tree.por.pruned,newdata=por.train)
tree.pruned.por.pred<-predict(tree.por.pruned,newdata=,por.test)
tree.pruned.por.pred
#MSE
mean((train.por-por.train$grade.con)^2)
mean((tree.pruned.por.pred-por.test$grade.con)^2)
```

Random Forrest regression
```{r}
library(randomForest)
library(caret)
### math--------
#choice of m
37/3
#cv on number of trees
test.error<-rep(1,10)
set.seed(114514)
folds<-createFolds(1:nrow(math.train),k=10,returnTrain=FALSE)
tree.vec<-seq(50,1000,by=50)
tree.error<-rep(1,length(tree.vec))
for(k in 1:length(tree.vec)){
  for(i in 1:10){
    set.seed(114514)
    train.rand.for.math<-randomForest(regression.formula,data=math.train[-folds[[i]],],mtry=12,ntree=k)
    train.pred.rand.for.math<-predict(train.rand.for.math,newdata=math.train[folds[[i]],])
    test.error[i]<-mean((train.pred.rand.for.math-math.train[folds[[i]],]$grade.con)^2)
  }
  tree.error[k]<-mean(test.error)
}
#the choice of number of trees
tree.vec[which.min(tree.error)]
best.rd.math<-tree.vec[which.min(tree.error)]
set.seed(114514)
train.rand.for.math<-randomForest(regression.formula,data=math.train,mtry=14,ntree=best.rd.math)
train.rand.for.math
test.pred.rand.for.math<-predict(train.rand.for.math,newdata=math.test)
train.pred.rand.for.math<-predict(train.rand.for.math,newdata=math.train)
#test result
test.pred.rand.for.math
#misclassifictaion rate
mean((train.pred.rand.for.math-math.train$grade.con)^2)
mean((test.pred.rand.for.math-math.test$grade.con)^2)
library(vip)
vip::vip(train.rand.for.math)


### por---------
#choice of m
37/3
#cv on number of trees
test.error<-rep(1,10)
set.seed(114514)
folds<-createFolds(1:nrow(por.train),k=10,returnTrain=FALSE)
tree.vec<-seq(50,1000,by=50)
tree.error<-rep(1,length(tree.vec))
for(k in 1:length(tree.vec)){
  for(i in 1:10){
    set.seed(114514)
    train.rand.for.por<-randomForest(regression.formula,data=por.train[-folds[[i]],],mtry=12,ntree=k)
    train.pred.rand.for.por<-predict(train.rand.for.por,newdata=por.train[folds[[i]],])
    test.error[i]<-mean((train.pred.rand.for.por-por.train[folds[[i]],]$grade.con)^2)
  }
  tree.error[k]<-mean(test.error)
}
#the choice of number of trees
tree.vec[which.min(tree.error)]
best.rd.por<-tree.vec[which.min(tree.error)]
set.seed(114514)
train.rand.for.por<-randomForest(regression.formula,data=por.train,mtry=14,ntree=best.rd.por)
train.rand.for.por
test.pred.rand.for.por<-predict(train.rand.for.por,newdata=por.test)
train.pred.rand.for.por<-predict(train.rand.for.por,newdata=por.train)
#test result
test.pred.rand.for.por
#misclassifictaion rate
mean((train.pred.rand.for.por-por.train$grade.con)^2)
mean((test.pred.rand.for.por-por.test$grade.con)^2)
library(vip)
vip::vip(train.rand.for.por)
```

TreeBased Classification

```{r}
library(rpart.plot)
library(rattle)
library(rpart)
library(caret)
library(gbm)
## Decision Tree for math---------
### Unpruned Tree----------
tree.math<- rpart(classification.formula, data=math.train,control=rpart.control(cp=0))
#predicted result for tree
tree.pred.math<-predict(tree.math,newdata=math.test,type="class")
tree.pred.math.train<-predict(tree.math,newdata=math.train,type="class")
tree.pred.math
#misclassification for unpruned
1-mean(tree.pred.math.train==math.train$grade.cat)
1-mean(tree.pred.math==math.test$grade.cat)
### Pruned Tree-------
set.seed(114514)
tree.tune.math <- train(classification.formula, 
                    data = math.train, 
                  method = "rpart", 
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cp = seq(0, 0.05, by=0.001)),
                  trControl = trainControl(
                    method = "repeatedcv",
                    repeats = 5, number = 10
                    )
                 )
plot(tree.tune.math)
#best cost parameter
best<-tree.tune.math$bestTune
best
tree.math.pruned<- rpart(classification.formula, data=math.train,control=rpart.control(cp=best))
## We can't plot this because it's only a root--------
#predicted result for pruned tree
tree.pruned.math.pred<-predict(tree.math.pruned,newdata=math.test,type="class")
tree.pruned.math.train<-predict(tree.math.pruned,newdata=math.train,type="class")
tree.pruned.math.pred
#misclassification for pruned
1-mean(tree.pruned.math.train==math.train$grade.cat)
1-mean(tree.pruned.math.pred==math.test$grade.cat)



## Decision Tree for Por---------
tree.por<- rpart(classification.formula, data=por.train,control=rpart.control(cp=0))
#predicted result for tree
tree.pred.por<-predict(tree.por,newdata=por.test,type="class")
tree.pred.por.train<-predict(tree.por,newdata=por.train,type="class")
tree.pred.por
#misclassification rate for unpruned
1-mean(tree.pred.por.train==por.train$grade.cat)
1-mean(tree.pred.por==por.test$grade.cat)
### Pruned Tree-------
set.seed(114514)
tree.tune.por <- train(classification.formula, 
                        data = por.train, 
                        method = "rpart", 
                        preProcess = c("center", "scale"),
                        tuneGrid = data.frame(cp = seq(0, 0.2, by=0.004)),
                        trControl = trainControl(
                          method = "repeatedcv",
                          repeats = 5, number = 10
                        )
)
plot(tree.tune.por)
#best cost parameter
best<-tree.tune.por$bestTune
best
tree.por.pruned<- rpart(classification.formula, data=por.train,control=rpart.control(cp=best))
fancyRpartPlot(tree.por.pruned, caption = NULL)
#predicted result for pruned tree
tree.pruned.por.pred<-predict(tree.por.pruned,newdata=por.test,type="class")
tree.pruned.por.train<-predict(tree.por.pruned,newdata=por.train,type="class")
tree.pruned.por.pred
#misclassification rate for pruned
1-mean(tree.pruned.por.train==por.train$grade.cat)
1-mean(tree.pruned.por.pred==por.test$grade.cat)
```

Random Forrest Classification
```{r}
library(randomForest)
### math--------
#choice of m
sqrt(37)
#cv on number of trees
test.error<-rep(1,10)
set.seed(114514)
folds<-createFolds(1:nrow(math.train),k=10,returnTrain=FALSE)
tree.vec<-seq(50,1000,by=50)
tree.error<-rep(1,length(tree.vec))
for(k in 1:length(tree.vec)){
  for(i in 1:10){
    set.seed(114514)
    train.rand.for.math<-randomForest(classification.formula,data=math.train[-folds[[i]],],mtry=6,ntree=k)
    train.pred.rand.for.math<-predict(train.rand.for.math,newdata=math.train[folds[[i]],],type="class")
    test.error[i]<-1-mean(train.pred.rand.for.math==math.train[folds[[i]],]$grade.cat)
  }
  tree.error[k]<-mean(test.error)
}
#the choice of number of trees
tree.vec[which.min(tree.error)]
best.rd.math<-tree.vec[which.min(tree.error)]
set.seed(114514)
train.rand.for.math<-randomForest(classification.formula,data=math.train,mtry=6,ntree=best.rd.math)
train.rand.for.math
test.pred.rand.for.math<-predict(train.rand.for.math,math.test,type="class")
train.pred.rand.for.math<-predict(train.rand.for.math,math.train,type="class")
#test result
test.pred.rand.for.math
#misclassifictaion rate
1-mean(train.pred.rand.for.math==math.train$grade.cat)
1-mean(test.pred.rand.for.math==math.test$grade.cat)
library(vip)
vip::vip(train.rand.for.math)
### por---------
### por--------
#choice of m
sqrt(37)
#cv on number of trees
test.error<-rep(1,10)
set.seed(114514)
folds<-createFolds(1:nrow(por.train),k=10,returnTrain=FALSE)
tree.vec<-seq(50,1000,by=50)
tree.error<-rep(1,length(tree.vec))
for(k in 1:length(tree.vec)){
  for(i in 1:10){
    set.seed(114514)
    train.rand.for.por<-randomForest(classification.formula,data=por.train[-folds[[i]],],mtry=6,ntree=k)
    train.pred.rand.for.por<-predict(train.rand.for.por,newdata=por.train[folds[[i]],],type="class")
    test.error[i]<-1-mean(train.pred.rand.for.por==por.train[folds[[i]],]$grade.cat)
  }
  tree.error[k]<-mean(test.error)
}
#the choice of number of trees
tree.vec[which.min(tree.error)]
best.rd.por<-tree.vec[which.min(tree.error)]
set.seed(114514)
train.rand.for.por<-randomForest(classification.formula,data=por.train,mtry=6,ntree=best.rd.por)
train.rand.for.por
test.pred.rand.for.por<-predict(train.rand.for.por,por.test,type="class")
train.pred.rand.for.por<-predict(train.rand.for.por,por.train,type="class")
#test result
test.pred.rand.for.por
#misclassifictaion rate
1-mean(train.pred.rand.for.por==por.train$grade.cat)
1-mean(test.pred.rand.for.por==por.test$grade.cat)
library(vip)
vip::vip(train.rand.for.por)
```

